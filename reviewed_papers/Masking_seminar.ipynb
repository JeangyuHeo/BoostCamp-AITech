{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129b3480-43be-4369-bf93-2730cd6e2ec0",
   "metadata": {},
   "source": [
    "# 세미나 자료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b2c563-6a81-4cf7-96ef-e57a33d7493a",
   "metadata": {},
   "source": [
    "기본적으로 offset_mapping과 overflow_to_sample_mapping에 대한 개념을 알고 있다고 가정하고 세미나를 진행합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59d8c9f3-9279-4716-acd8-bbf876b825c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import load_from_disk, DatasetDict, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from konlpy.tag import Mecab\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2be682-47f9-403e-be55-bc4525800636",
   "metadata": {},
   "source": [
    "### 사용될 constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3a55a93-5cf2-46d4-9590-f6d09edc172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=384\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f3302-1ed7-4362-a9ef-547e74a9c37b",
   "metadata": {},
   "source": [
    "### Question과 Context에 대해서 Embedding 값을 구해줄 Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e77c6502-fec5-4d90-b281-57f229db1416",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiyoung2/roberta-large-qaconv-sds were not used when initializing RobertaModel: ['qa_outputs.convs.0.layer_norm.weight', 'qa_outputs.convs.0.layer_norm.bias', 'qa_outputs.convs.2.conv2.bias', 'qa_outputs.convs.1.conv2.weight', 'qa_outputs.convs.1.conv1.weight', 'qa_outputs.convs.3.conv2.bias', 'qa_outputs.convs.4.conv1.bias', 'qa_outputs.convs.3.conv1.weight', 'qa_outputs.convs.2.conv1.bias', 'qa_outputs.convs.3.conv2.weight', 'qa_outputs.convs.2.layer_norm.weight', 'qa_outputs.convs.2.conv1.weight', 'qa_outputs.convs.3.conv1.bias', 'qa_outputs.convs.0.conv1.bias', 'qa_outputs.convs.3.layer_norm.bias', 'qa_outputs.qa_output.bias', 'qa_outputs.convs.4.conv2.weight', 'qa_outputs.convs.2.conv2.weight', 'qa_outputs.convs.4.conv1.weight', 'qa_outputs.convs.1.conv1.bias', 'qa_outputs.convs.0.conv1.weight', 'qa_outputs.convs.1.layer_norm.weight', 'qa_outputs.convs.0.conv2.weight', 'qa_outputs.convs.4.layer_norm.bias', 'qa_outputs.convs.3.layer_norm.weight', 'qa_outputs.convs.0.conv2.bias', 'qa_outputs.convs.4.conv2.bias', 'qa_outputs.convs.1.conv2.bias', 'qa_outputs.convs.4.layer_norm.weight', 'qa_outputs.convs.1.layer_norm.bias', 'qa_outputs.convs.2.layer_norm.bias', 'qa_outputs.qa_output.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at kiyoung2/roberta-large-qaconv-sds and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at kiyoung2/roberta-large-qaconv-sds were not used when initializing RobertaModel: ['qa_outputs.convs.0.layer_norm.weight', 'qa_outputs.convs.0.layer_norm.bias', 'qa_outputs.convs.2.conv2.bias', 'qa_outputs.convs.1.conv2.weight', 'qa_outputs.convs.1.conv1.weight', 'qa_outputs.convs.3.conv2.bias', 'qa_outputs.convs.4.conv1.bias', 'qa_outputs.convs.3.conv1.weight', 'qa_outputs.convs.2.conv1.bias', 'qa_outputs.convs.3.conv2.weight', 'qa_outputs.convs.2.layer_norm.weight', 'qa_outputs.convs.2.conv1.weight', 'qa_outputs.convs.3.conv1.bias', 'qa_outputs.convs.0.conv1.bias', 'qa_outputs.convs.3.layer_norm.bias', 'qa_outputs.qa_output.bias', 'qa_outputs.convs.4.conv2.weight', 'qa_outputs.convs.2.conv2.weight', 'qa_outputs.convs.4.conv1.weight', 'qa_outputs.convs.1.conv1.bias', 'qa_outputs.convs.0.conv1.weight', 'qa_outputs.convs.1.layer_norm.weight', 'qa_outputs.convs.0.conv2.weight', 'qa_outputs.convs.4.layer_norm.bias', 'qa_outputs.convs.3.layer_norm.weight', 'qa_outputs.convs.0.conv2.bias', 'qa_outputs.convs.4.conv2.bias', 'qa_outputs.convs.1.conv2.bias', 'qa_outputs.convs.4.layer_norm.weight', 'qa_outputs.convs.1.layer_norm.bias', 'qa_outputs.convs.2.layer_norm.bias', 'qa_outputs.qa_output.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at kiyoung2/roberta-large-qaconv-sds and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "p_encoder = AutoModel.from_pretrained('kiyoung2/roberta-large-qaconv-sds', use_auth_token=True).to(device)\n",
    "q_encoder = AutoModel.from_pretrained('kiyoung2/roberta-large-qaconv-sds', use_auth_token=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee0a23-8ce3-4ae9-9070-680714bad927",
   "metadata": {},
   "source": [
    "### Masked Dataset을 만들어줘!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8d557e0-e7ae-4ae0-926d-957eac37a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_emb_mask_dataset(dataset_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('kiyoung2/roberta-large-qaconv-sds', use_auth_token=True)\n",
    "    \n",
    "    # 데이터 불러오기!!\n",
    "    raw_train_dataset = load_from_disk(os.path.join(dataset_path, \"train_dataset\"))['train']\n",
    "    raw_val_dataset = load_from_disk(os.path.join(dataset_path, \"train_dataset\"))['validation']\n",
    "    \n",
    "    # 비효율적이야... 하지만 offset_mapping과 sample_mapping이 필요해..\n",
    "    tokenized_c = tokenizer(raw_train_dataset['context'], return_tensors='pt', truncation=True, max_length=max_seq_length, stride=128,return_overflowing_tokens=True,return_offsets_mapping=True,padding=\"max_length\")\n",
    "    \n",
    "    offset_mapping = tokenized_c.pop(\"offset_mapping\")\n",
    "    sample_mapping = tokenized_c.pop(\"overflow_to_sample_mapping\")\n",
    "    \n",
    "    # 영차영차! 데이터를 만들자!! \n",
    "    ext_prepare_train_features_flatten_trunc = partial(_ext_prepare_train_features_flatten_trunc, tokenizer = tokenizer)\n",
    "    new_q = raw_train_dataset.map(\n",
    "        ext_prepare_train_features_flatten_trunc,\n",
    "        batched=True,\n",
    "        remove_columns=raw_train_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    # batch 단위의 처리를 위해서 Dataloader를 만들어보자!!\n",
    "    Tensor_dataset = TensorDataset(\n",
    "        tokenized_c['input_ids'], tokenized_c['attention_mask'], tokenized_c['token_type_ids'],\n",
    "        torch.tensor(new_q['ids']), torch.tensor(new_q['attention']), torch.tensor(new_q['token_type']),\n",
    "        torch.tensor(new_q['answer'])\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        Tensor_dataset,\n",
    "        batch_size=2\n",
    "    )\n",
    "    \n",
    "    # 데이터를 만들었으니 Masking을 해보자!!\n",
    "    masked_dataset = mask_word_with_emb(train_dataloader, tokenizer, offset_mapping, sample_mapping, raw_train_dataset)\n",
    "    \n",
    "    new_dataset = DatasetDict({\n",
    "        'train':masked_dataset,\n",
    "        'validation':raw_val_dataset\n",
    "    })\n",
    "    \n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aada28e-3fca-4eed-b531-973b1805273a",
   "metadata": {},
   "source": [
    "### Truncation! 때문에 Question과 context, answer에 mapping 하는게 힘들어!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddd4fe3b-d57d-47a9-925b-ede669760c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ext_prepare_train_features_flatten_trunc(examples, tokenizer):\n",
    "    new_tokenized_ids=[]\n",
    "    new_att =[]\n",
    "    new_token_type=[]\n",
    "    new_answer = []\n",
    "    \n",
    "    texts = [text['text'][0] for text in examples['answers']]\n",
    "    \n",
    "    tokenized_q = tokenizer(examples['question'], return_tensors='pt', truncation=True, max_length=max_seq_length, padding=\"max_length\")\n",
    "    tokenized_c = tokenizer(examples['context'], return_tensors='pt', truncation=True, max_length=max_seq_length, stride=128,return_overflowing_tokens=True,return_offsets_mapping=True,padding=\"max_length\")\n",
    "    tokenized_a = tokenizer(texts, return_tensors='pt', max_length=100, padding=\"max_length\")\n",
    "    \n",
    "    sample_mapping = tokenized_c.pop(\"overflow_to_sample_mapping\")\n",
    "    \n",
    "    # truncation된 데이터에 맞춰서 answer와 question를 추가해주자!!\n",
    "    for i in tqdm(sample_mapping):\n",
    "        new_tokenized_ids.append(tokenized_q['input_ids'][i].tolist())\n",
    "        new_att.append(tokenized_q['attention_mask'][i].tolist())\n",
    "        new_token_type.append(tokenized_q['token_type_ids'][i].tolist())\n",
    "        new_answer.append(tokenized_a['input_ids'][i].tolist())\n",
    "    \n",
    "    return { 'ids':new_tokenized_ids,\n",
    "            'attention':new_att,\n",
    "            'token_type':new_token_type,\n",
    "            'answer':new_answer }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f887e39e-a8dd-464b-b01f-7be81df8f8ea",
   "metadata": {},
   "source": [
    "### 본격적으로 Masking 해볼까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6edf71dd-1a04-4851-8e6a-6b021afa592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_word_with_emb(dataloader, tokenizer, offset_mapping, sample_mapping, train_dataset):\n",
    "    new_ids=[]\n",
    "    mask_token = tokenizer.mask_token_id\n",
    "    \n",
    "    # similarity를 계산할 때, 계산되면 안되는 special token들\n",
    "    ignore_tokens = [tokenizer.pad_token_id, \n",
    "                     tokenizer.unk_token_id,\n",
    "                     tokenizer.cls_token_id, \n",
    "                     tokenizer.sep_token_id]\n",
    "    \n",
    "    with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "        for batch in tepoch:\n",
    "            labels = batch[0].clone()\n",
    "            \n",
    "            # tokenized context 정보\n",
    "            p_inputs={\n",
    "                \"input_ids\": batch[0].to(device),\n",
    "                \"attention_mask\": batch[1].to(device),\n",
    "                \"token_type_ids\": batch[2].to(device)\n",
    "            }\n",
    "            \n",
    "            # tokenized question 정보\n",
    "            q_inputs = {\n",
    "                \"input_ids\": batch[3].to(device),\n",
    "                \"attention_mask\": batch[4].to(device),\n",
    "                \"token_type_ids\": batch[5].to(device)\n",
    "            }\n",
    "            \n",
    "            # masking하려는 단어가 answer인지 아닌지 판단하려면 필요하겠지?\n",
    "            answers = [tokenizer.decode(i, skip_special_tokens=True) for i in batch[6]]\n",
    "            \n",
    "            # 위에 선언했던 무시해야 할 special token의 위치를 표시할 matrix\n",
    "            matrix = torch.full(labels.shape, True)\n",
    "            \n",
    "            # list에 포함된 token이랑 같으면 matrix의 값을 False로 바꾸자!\n",
    "            for ignore_token in ignore_tokens:\n",
    "                ignore_mask = labels.eq(ignore_token)\n",
    "                matrix.masked_fill_(ignore_mask, value=False)\n",
    "            \n",
    "            # 0 -> 각 token들에 대한 embedding 값\n",
    "            # 1 -> 전체에 대한 embedding 값\n",
    "            p_outputs = p_encoder(**p_inputs)[0]\n",
    "            q_outputs = q_encoder(**q_inputs)[1]\n",
    "            \n",
    "            batch_size = p_outputs.shape[0]\n",
    "            \n",
    "            # (batch_size, 1024) -> (batch_size, 1, 1024)\n",
    "            q_outputs = q_outputs.view(batch_size,1,-1)\n",
    "            # (batch_size, 384, 1024) -> (batch_size, 1024, 384)\n",
    "            p_outputs = torch.transpose(p_outputs.view(batch_size, max_seq_length, -1), 1, 2)\n",
    "            \n",
    "            #batch 단위 matmul\n",
    "            #(batch_size, 1, 384) -> (batch_size, 384)\n",
    "            sim_scores = torch.bmm(q_outputs, p_outputs).squeeze()\n",
    "            # 모양이 제대로 잡혔지만, 확인 차 !\n",
    "            # (batch_size, 384) -> (batch_size, 384)\n",
    "            sim_scores = sim_scores.view(batch_size, -1)\n",
    "            # special token 값이 softmax에 반영되지 않게!\n",
    "            sim_scores[~matrix] = -100\n",
    "            # 자~ 구해보자~ similarity!!\n",
    "            sim_scores = F.log_softmax(sim_scores, dim=1)\n",
    "            \n",
    "            # similarity가 큰 순서대로 sorting을 해주고, 점수가 큰거부터 masking!\n",
    "            for idx, score in enumerate(sim_scores):\n",
    "                sorted_score, sorted_idx = torch.sort(score, descending=True)\n",
    "                for i in range(2):\n",
    "                    p_inputs['input_ids'][idx] = make_mask_word(tokenizer, p_inputs['input_ids'][idx], answers[idx], sorted_idx[i])\n",
    "                new_ids.append(p_inputs['input_ids'][idx].tolist())\n",
    "    \n",
    "    #어!? masking 해주니까 answer 위치가 바뀌네!? 잡아주자!\n",
    "    origin_answers = train_dataset['answers']\n",
    "    origin_start = [data['answer_start'][0] for data in train_dataset['answers']]\n",
    "    origin_text = train_dataset['context']\n",
    "    \n",
    "    # token 단위에서 처리한 masking을 원래 dataset에 반영해주자!\n",
    "    for list_idx, offset_list in enumerate(offset_mapping):\n",
    "        for idx, offsets in enumerate(offset_list):\n",
    "            # 같은 거는 special token\n",
    "            if offsets[0] == offsets[1]:\n",
    "                continue\n",
    "            else:\n",
    "                #아닐 경우 mask!\n",
    "                if new_ids[list_idx][idx] == tokenizer.mask_token_id:\n",
    "                    for i in range(offsets[0], offsets[1]):\n",
    "                        origin_list_idx = sample_mapping[list_idx]\n",
    "                        origin_text[origin_list_idx] = list(origin_text[origin_list_idx])\n",
    "                        origin_text[origin_list_idx][i] = '∬'\n",
    "                        origin_text[origin_list_idx] = \"\".join(origin_text[origin_list_idx])\n",
    "\n",
    "    # mask 위치를 다시 잡아주자!\n",
    "    for i, text in enumerate(origin_text):\n",
    "        origin_text[i] = text[:origin_start[i]] + '[ans]' + text[origin_start[i]:]\n",
    "    \n",
    "    # 편- 안-\n",
    "    for idx, origin in enumerate(origin_text):\n",
    "        origin_text[idx] = re.sub('∬+',tokenizer.mask_token, origin)\n",
    "        origin_answers[idx]['answer_start'] = [origin_text[idx].find('[ans]')]\n",
    "        origin_text[idx] = re.sub('\\[ans\\]','', origin_text[idx])\n",
    "    \n",
    "    # 휴 끝났어유~\n",
    "    return Dataset.from_dict({\n",
    "        'title': train_dataset['title'],\n",
    "        'context': origin_text,\n",
    "        'question': train_dataset['question'],\n",
    "        'id': train_dataset['id'],\n",
    "        'answers': origin_answers,\n",
    "        'document_id': train_dataset['document_id'],\n",
    "        '__index_level_0__' : train_dataset['__index_level_0__'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5802fcdc-e8a3-4c06-9338-292ea768f96a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### token이 포함된 단어를 찾아보자!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17c5353d-337b-419a-b1ed-de188fbce164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask_word(tokenizer, ids, answer, idx):\n",
    "    # 와 ~ tensor 값은 -, + 안되는지 모르고 한참을 디버깅했다..\n",
    "    front_idx = int(idx)\n",
    "    back_idx = int(idx)\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    \n",
    "    # 단어 시작점을 찾아주자!\n",
    "    while True:\n",
    "        if tokens[front_idx][:2] == \"##\":\n",
    "            front_idx -= 1\n",
    "        elif (len(tokens[front_idx])<=2 or tokens[front_idx][:2]!=\"##\"):\n",
    "            break\n",
    "        else: \n",
    "            front_idx -= 1\n",
    "    # 단어 끝점을 찾아주자!\n",
    "    while True:\n",
    "        if (len(tokens[back_idx+1])<=2) or (tokens[back_idx+1][:2]!=\"##\"):\n",
    "            break\n",
    "        else:\n",
    "            back_idx+=1\n",
    "    \n",
    "    # ##는 시져시져~\n",
    "    word =  re.sub('##','',''.join(tokens[front_idx:back_idx+1]))\n",
    "    \n",
    "    # 정답이 아니면 masking!!!!! 받아랏!\n",
    "    if answer not in word:\n",
    "        for idx in range(front_idx,back_idx+1):\n",
    "            tokens[idx] = tokenizer.mask_token\n",
    "    \n",
    "    result = torch.tensor(tokenizer.convert_tokens_to_ids(tokens))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f866fb1b-15a4-4eb0-b8c6-37f78839ab9f",
   "metadata": {},
   "source": [
    "## 자 돌려보자!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4410789-1fab-4db5-9c21-5a54b225b328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at data/aistage-mrc/train_dataset/train/cache-d171d7709cac2c45.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f1e9a3ff874ca7ae3b1a122431e340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3647 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_dataset = make_emb_mask_dataset(\"./data/aistage-mrc/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34494d-defa-4f3a-ad5c-8c67068ea10d",
   "metadata": {},
   "source": [
    "## 너도 이제 마스킹 고수!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc67eb1f-ccd4-4b86-ae28-911f12d259a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
       "        num_rows: 3952\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
       "        num_rows: 240\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e881f66f-2618-4fe0-ad19-93c7bfd84e2f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['미국 상의원 또는 미국 상원(United States Senate)은 [MASK] 미국 의회의 상원이다. 미국 부통령이 상원의장이 된다. 각 주당 2명의 상원의원이 선출되어 100명의 상원의원으로 구성되어 있다. 임기는 6년이며, 2년마다 50개주 중 1/3씩 상원의원을 새로 선출하여 연방에 보낸다. 미국 상원은 미국 하원과는 다르게 미국 대통령을 수반으로 하는 미국 연방 행정부에 각종 동의를 하는 기관이다. 하원이 세금과 경제에 대한 권한, 대통령을 포함한 대다수의 공무원을 파면할 권한을 갖고 있는 국민을 대표하는 기관인 반면 상원은 미국의 주를 대표한다. 즉 캘리포니아주, 일리노이주 같이 주 정부와 주 의회를 대표하는 기관이다. 그로 인하여 군대의 파병, 관료의 임명에 대한 동의, 외국 조약에 대한 승인 등 신속을 요하는 권한은 모두 상원에게만 있다. 그리고 하원에 대한 견제 역할(하원의 법안을 거부할 권한 등)을 담당한다. 2년의 임기로 인하여 급진적일 수밖에 없는 하원은 지나치게 급진적인 법안을 만들기 쉽다[MASK] 대표적인 예로 건강보험 개혁 당시 하원이 미국 연방 행정부에게 퍼블릭 옵션(공공건강보험기관)의 조항이 있는 반면 상원의 경우 하원안이 지나치게 세금이 많이 든다는 이유로 [MASK] 옵션 조항을 제외하고 비영리건강보험기관이나 보험회사가 담당하도록 한 것이다. 이 경우처럼 상원은 하원이나 내각책임제가 빠지기 쉬운 국가들의 국회처럼 걸핏하면 발생하는 의회의 비정상적인 사태를 [MASK] 기관이다. 상원은 급박한 처리사항의 경우가 아니면 법안을 먼저 내는 경우가 드물고 하원이 만든 법안을 수정하여 다시 하원에 되돌려보낸다. 이러한 방식으로 단원제가 빠지기 쉬운 함정을 미리 방지하는 것이다.날짜=2017-02-05',\n",
       " \"'근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 1950년대이다. 2차 세계대전을 마치고, 6.25전쟁의 시기로 유럽은 전후 재건에 집중하고, 유럽 제국주의의 식민지가 독립하여 아프리카, 아시아, 아메리카 대륙에서 신생국가가 형성되는 시기였고, 미국은 전쟁 이후 경제적 변화에 기업이 적응을 해야 하던 시기였다. 특히 1954년 피터 드러커의 저서 《경영의 실제》는 현대적 경영의 기준을 제시하여서, 기존 근대적 인사조직관리를 넘어선 현대적 인사조직관리의 전환점이 된다. 드러커는 경영자의 역할을 강조하며 경영이 현시대 최고의 예술이자 과학이라고 주장하였고 , 이 주장은 21세기 인사조직관리의 역할을 자리매김했다. 현대적 인사조직관리와 근대 인사조직관리의 가장 큰 차이는 통합이다. 19세기의 영향을 받던 근대적 경영학(고전적 경영)의 흐름은 기능을 강조하였지만, [MASK] 이후의 현대 경영학은 통합을 강조하였다. 기능이 분화된 '기계적인 기업조직' 이해에서 다양한 기능을 인사조직관리의 목적, 경영의 목적을 위해서 다양한 분야를 통합하여 '유기적 기업 조직' 이해로 전환되었다. 이 통합적 접근방식은 과정, 시스템, 상황을 중심으로 하는 인사조직관리 방식을 형성했다[MASK]\",\n",
       " '강희제는 강화된 황권으로 거의 황제 중심의 독단적으로 나라를 이끌어 갔기에 자칫 전제 독재의 가능성이 보일 수도 있었으나, 스스로 황권을 조절하고 정치의 일부는 재상들이나 대신들과 의논하였으며 당시 궁핍하게 살고 있는 한족들의 사정을 잘 알고 있던 한족 대신들의 의견을 수용하여 정책을 실행하고 선정을 베풀었다. 프랑스의 예수회 선교사 부베는 루이 14세에게 다음과 같이 보고하였다. 강희제는 세상에서 가장 부유한 군주입니다. 그럼에도 황제인 그의 생활용품들은 사치스러움과 화려함과는 거리가 멀다 못해 소박하기 그지없습니다. 역대 제왕들 가운데 전례없는 일입니다. 강희제 스스로도 자신이 직접 쓴 《근검록》에서 다음과 같이 쓰고 있다 모든 비용은 백성들의 피땀으로 얻어진 것이니 주인된 황제로서 절제하고 절제함은 당연한 것이 아닌가 이런 강희제의 인자한 정치는 한족이 만주족의 청나라를 지지하게 만드는 데에 크게 일조하였다. 1717년(강희 56년) 강희제는 〈고별상유〉([MASK]別上諭), 즉 마지막으로 백성들에게 바치는 글을 남겼는데 강희제는 “한 가지 일에 부지런하지 않으면 온 천하에 근심을 끼치고, 한 순간에 부지런하지 않으면 천추만대에 우환거리를 남긴다.”라고 역설하였다. 또한 “제왕이 천하를 다스림에 능력이 있는 자를 가까이 두고, 백성들의 세금을 낮추어 주어야 하며, 백성들의 마음을 하나로 [MASK], 위태로움이 생기기 전에 나라를 보호하며, 혼란이 있기 전에 이를 [MASK] 파악하여 잘 다스리고, 관대하고 엄격함의 조화를 이루어 나라를 위한 계책을 도모해야 한다.”라고 후대의 황제에게도 이를 훈계하였다. 강희제는 황제로서 자식과 같은 백성들에게 이런 당부의 말을 남겨 황제로서의 도리를 다하려 하였다[MASK]',\n",
       " '불상을 모시기 위해 나무나 돌, 쇠 등을 깎아 일반적인 건축물보다 작은 규모로 만든 것을 불감(佛龕)이라고 한다. 불감은 그 안에 모신 불상의 양식뿐만 아니라, [MASK] 건축 양식을 함께 살필 수 있는 중요한 자료가 된다. 이 작품은 높이 18cm의 작은 불감으로, 청동으로 불감과 불상을 만들고 그 위에 금칠을 하였다. 불감 내부를 살펴보면 난간을 두른 사각형의 기단 위에 본존불과 양 옆에 보살상이 있으며, 그 위에 기둥과 지붕으로 된 뚜껑이 덮혀 있다. 법당 모양의 뚜껑에는 앞면과 양쪽에 커다란 창문이 있어서 안에 모셔진 불상을 잘 볼 수 있도록 하였다. 본존불은 얼굴이 추상적이고, 양 어깨를 감싸고 있는 옷은 주름을 간략한 선으로 표현했다. 몸 뒤편에 있는 광배(光背)는 머리광배와 몸광배로 나누어져 있으며, 불꽃무늬로 가장자리를 장식하고 있다. 본존불 양 옆의 보살상도 구슬로 장식된 관(冠)을 쓰고 있다는 점을 제외하면 형식이나 표현 수법이 본존불과 유사하다. 불감은 지금도 금색이 찬란하고 지붕에 녹청색이 남아 있는 등 전체적인 보존 상태가 양호하다. 본존불의 긴 허리, 불규칙하게 나타나는 옷주름, 그리고 보살이 쓰고 있는 구슬로 장식한 관(冠) 등 여러 양식으로 보아 만든 시기는 중국 북방 계통의 영향을 받은 1112세기 경으로 추정된다[MASK] 이 작품은 고려시대 또는 그 [MASK] 목조건축 양식과 조각수법을 보여주는 귀중한 예라는 점에서 가치가 크다고 할 수 있다[MASK]',\n",
       " '동아대학교박물관에서 소장하고 있는 계사명 사리구는 총 4개의 용기로 구성된 조선후기의 유물로, 경상남도 울주군 웅촌면 대복리에서 출토되었다고 전한다. 외함(外函)은 청화명문이 있는 백자이며, 그 안쪽에 납작한 금속제 원형 합 2점과 금속제 원통형 합 등 3점의 그릇이 봉안되어 있다. 바깥쪽의 외함인 백자 합 동체 중앙부 표면에 청화안료로 쓴 “癸巳二月日 施主承表 兩主”라는 명문이 세로로 세 줄에 걸쳐서 쓰여 있어 조선 후기인 계사년에 시주자인 승표 부부가 발원하여 만든 것임을 알 수 있다. 동아대학교박물관의 계사명 사리구는 정확한 제작연대는 알 수 없지만 명문 등을 통해 적어도 17세기 이후에 제작된 것으로 추정되는 작품으로, 명문이 있는 조선 후기 경상도 지역 출토 사리장엄구라는 점에서 중요한 가치를 지닌 작품으로 판단된다. 조선 후기 사리장엄구는 아직까지 조사와 연구가 거의 이루어지지 않았으나, 이처럼 세트를 갖추어 출토된 유물은 비교적 드문 편임을 [MASK] 때, 이 계사명 사리장엄구는 제작연대와 발원자의 이름이 밝혀져 있으며, 지금까지 출토된 예가 드문 비교적 완전한 [MASK] 가진 유물이라는 점에서 조선 후기 사리장엄구 연구에 자료적 가치를 지닌 유물이다.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dataset['train']['context'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a8c65-d857-4eaa-a380-dba854ec2754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
