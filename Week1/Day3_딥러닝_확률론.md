### 딥러닝의 학습 방법

선형 모델과 같은 단순한 모델은 분류(Classification)와 같은 복잡한 패턴의 모델을 예측하기가 어렵다.



**비선형 모델을 통해 알아가보자.**

* **비선형 모델**이라 할지라도 내부적으로는 **선형 모델을 활성화 함수(activation function)를 통해** 만들어진 것이다.

<img width="558" alt="image-20210808152542712" src="https://user-images.githubusercontent.com/88299729/130345146-846a035c-74a9-44aa-9982-7d25cd8d6748.png">

<선형 모델의 기본적인 모습)

*O* = *X* *W* +*b*

* *W* 는 데이터 X를 다른 차원으로 보내주는 역할을 한다고 생각하면 된다.
* Deep neural Network에서 화살표가 의미하는 것이 W이다.



### 소프트맥스(Softmax) 함수



<img width="398" alt="image-20210808153548286" src="https://user-images.githubusercontent.com/88299729/130345155-841e08da-e565-4068-812d-f35cc75c1204.png">



* Softmax 함수란 모델의 출력을 **확률로 바꾸어 나타내주는 연산**이다.

* **분류 문제**를 해결하기 위해 softmax 함수를 활용한다.

* 각 상황에 대해서 확률 형태로 값이 주어지고 그 값들의 **총합은 1**이다.

* **softmax는 학습 시**에만 사용되고, **추론을 할 시에는 원-핫 벡터(One-hot vector)**를 사용한다.

  * one-hot vector은 최대값만 1 나머지는 0으로 바꾸어 주는 작업을 한다.

  

<img width="662" alt="image-20210808154237293" src="https://user-images.githubusercontent.com/88299729/130345160-743dac62-257e-4565-bcad-09fb7b116a64.png">



<선형 함수의 **출력 값(실수 값)**을  입력 값으로 받아 **비선형 함수로 만드는 활성함수(activation function)**의 사용 모습>

* 이러한 기본적인 모습의 신경망을 역사적으로 **퍼셉트론(perceptron)** 이라고 부른다.



### 활성 함수 (Activation Function)



<img width="616" alt="image-20210808155243491" src="https://user-images.githubusercontent.com/88299729/130345166-32805646-fcbb-4296-9b67-a20e304510da.png">



**시그모이드 함수 (sigmoid) 나 하이퍼볼릭탄젠트 함수(tanh)의 경우 과거**에 많이 사용 되었지만, **오늘 날에는 ReLU함수**가 대부분의 경우에 사용된다.

* **Gradient Vanishing Problem** (미분 값 사라지는 문제)
  * Sigmoid와 tanh 함수의 경우, **미분의 최댓값이 작으며** 어느정도 **input 값이 작아지거나 커지게 되면 0으로 수렴**하게 된다. 이러한 경우가 발생하게 되면 **역전파(back propagation) 과정**에서 **미분 값이 소실 될 가능성**이 있다. 

*  **Sigmoid function은 함수 값의 중심이 0** 아니다.
  * 한 노드에 대해 모든 파라미터 w의 미분 값이 모두 같은 부호를 가지게 되고, 같은 방향으로 update 된다.  이러한 경향은 zigzag 형태로 학습을 하게 만들어 학습이 느려지게 된다.



### 신경망

<img width="659" alt="image-20210808163825428" src="https://user-images.githubusercontent.com/88299729/130345176-70f061a1-0989-493e-a2a5-b612498249c0.png">



< 위의 내용들을 토대로 만들어진 한 층의 신경망 >



#### 다층 신경망 (N-layer NN) 구현하기

* 두번째 층에서부터의 input은 이전 활성함수의 output을 사용하여 연산을 진행한다.

  * X 를 W<sub>1</sub>, b<sub>1</sub>를 통해 z라는 잠재 벡터를 만든다.
  * z를 activation function 연산을 하여 잠재 벡터 H를 만든다.
  * H를 W<sub>2</sub>, b<sub>2</sub> 를 통해 z<sub>2</sub>를 만든다.
  * 위의 연산을 반복한다.

  

<img width="648" alt="image-20210808164303932" src="https://user-images.githubusercontent.com/88299729/130345182-f298891d-0c13-4060-8923-6c738e0fa87a.png">



#### 여러 개의 층을 쌓는 이유

* 목적함수를 근사하는데 **필요한 뉴런(노드)의 개수가 확연하게 줄어**들어 **효율적인 학습이 가능**하다.
* 적은 수의 뉴런일지라도 Deep Neural Network를 사용하면 더 복잡한 함수를 표현 할 수 있다.



### 딥러닝 학습 원리



* #### 순전파 알고리즘 (Forward Propagation)

  * 주어진 입력이 왔을 때, 함수를 거쳐 출력을 내뱉는 과정

  

* #### 역전파 알고리즘 (Backward Propagation)

  * 결과적으로 우리가 알고 싶은 것은 Loss(원래 찾고자 하는 값과 예측 값의 차이)가 제일 작을 때의 W와 b 값이다. 
  * 이를 구하기 위해 **각 층의 가중치에 대한 gradient 벡터를 계산**해야한다.
  * 이때, 연산 수는 **W,b의 모든 원소의 개수와 같다**.
  * 연산에서 마지막 연산 층부터 처음 연산 층으로 내려오면서 업데이트가 이루어지는 방식이다. 이때 사용되는 것이 **연쇄 법칙**이다.



<img width="622" alt="image-20210808171607004" src="https://user-images.githubusercontent.com/88299729/130345188-ad4a1a94-1e13-42dd-a927-cc6ee3242a69.png">

<Back Prop이 되는 과정>



<img width="290" alt="image-20210808171655578" src="https://user-images.githubusercontent.com/88299729/130345195-a9b34402-bccc-496e-a00d-36f003ff8df4.png">

<연쇄 법칙>



### 확률론 기초 for AI

* #### 딥러닝에서 확률론이 필요한 이유

  * 딥러닝은 **확률론 기반의 기계학습 이론**에 바탕을 두고 있다.
  * 기계 학습에서 사용되는 손실함수(loss function)들은 **데이터 공간을 통계적으로 해석해서 유도**한 것이다. 이런 **손실함수를 가지고 모형들을 학습**시키므로, 확률론을 이해해야 딥러닝 모형 학습의 원리를 이해할 수 있다.
  * 기계학습에서 사용하는 손실함수는 **데이터 분포와 모델 예측 분포의 차이**, 즉 **예측이 틀릴 위험을 최소화하는 방향으로 학습**하도록 유도한다. 이 과정에서 **확률론을 기반으로 해석**한다.



확률 변수는 확률 분포에 따라 **이산형(discrete)**와 **연속형(continuous)**가 있다.



#### 이산형 (discrete, 확률 질량 함수)

* 확률이 연속적으로 분포가 되어있지 않으면 이산형이다. (실수여도 괜찮다.)
* 경우의 수를 다 더해서 확률을 구할 수 있다.
* P(X = x)는 확률 변수가 x값을 가질 확률이다.

<img width="251" alt="image-20210808175318645" src="https://user-images.githubusercontent.com/88299729/130345217-4073b1f5-c4cc-475d-90ee-0c3be7543516.png">



#### 연속형 (continuous, 확률 밀도 함수)

* 특정 x에서 가질 확률을 구하는 것이 불가능하다.
* 밀도를 통해 확률을 표시한다. P(x)는 확률이 아닌 누적확률분포의 변화율을 의미한다.

<img width="326" alt="image-20210808175909933" src="https://user-images.githubusercontent.com/88299729/130345225-8af73c1a-6c7f-46d1-87b5-86af6cb9730c.png">



#### 주변 확률 분포

P(X,Y)의 경우 **P(X) 처럼 X를 기준으로만 확률**을 구하거나 **P(Y)처럼 Y를 기준으로만 확률**을 구하는 것을 주변 확률 분포라고 한다.



#### 조건부 확률 분포

P(x|y)는 특정 y에 일어났을 때, x가 일어날 확률을 의미한다. 이는 입력 **x와 출력 y 사이의 관계**를 모델링 할 때 사용된다.

* Softmax 함수를 사용한 경우, 데이터에서 추출된 패턴을 기반으로 확률을 해석하는데에 사용된다.
* softmax는 데이터 x에 대한 특징 패턴과 가중치 행렬 *W* 를 통해 조건부 확률 P(y|x) 를 구한다.



#### 조건부 기댓값

**연속 확률 변수**를 다루는 경우, 밀도를 해석해야 된다. 이 경우 **조건부 확률이 아닌 조건부 기댓값을 추정**한다.

> 조건부 기댓값을 사용하는 이유

조건부기대값은 L_2-norm, 즉 E∥*y*−*f*(*x*)∥2를 최소화하는 함수 *f*(*x*)와 일치한다는 것이 수학적으로 증명되어있다.

* 일반적인 예측보다 좀 더 견실(robust)하게 예측하는 경우에는 조건부기대값보다는 **중앙값(median)을 사용해서 추정**하기도 한다.



#### 기댓값이란?



**기대값(expectation)**은 **데이터를 대표하는 통계량**이면서 동시에 확률분포를 통해 **다른 통계적 범함수를 계산**하는 데에 사용된다.

- 기댓값과 평균은 같은 용어로 많이들 사용하고 있는데, 기계학습에서는 좀 더 폭넓은 의미로 사용한다.

<img width="494" alt="image-20210808185445684" src="https://user-images.githubusercontent.com/88299729/130345233-8e2d2960-a877-43d4-99db-4afdcef86f24.png">



#### 몬테카를로 샘플링(Monte-Carlo Sampling)

* 확률 분포가 이산인지 연속인지 명시적으로 모를때 사용된다.
* Target f(x)에 데이터를 대입하여 샘플링한다.
* 분포에서 독립적으로 샘플링한다.
* 독립 추출이 보장된다면 대수의 법칙(law of large number)에 의해 수렴성을 보장한다.



#### 예시 : 적분하기

<img width="657" alt="image-20210808190908294" src="https://user-images.githubusercontent.com/88299729/130345243-781743f1-bcd2-4204-bdf4-75733b0a2f8d.png">

1. 구간 [-1, 1]의 길이는 2이므로 균등분포하여 샘플링한다. 확률분포로 바꾸기 위해 구간을 1씩 나누는 균등분포를 사용한다. 즉, 적분값을 2로 나눈다.
2. 이는 기대값을 계산하는 것과 같다. 따라서 몬테카를로 방법을 사용할 수 있다. 함수 e<sup>-x<sup>2</sup></sup>에 균등분포에서 추출한 데이터를 집어넣고, 상수평균을 구해준다. 이 값이 원하는 적분값의 1/2에 근사한 값이다.
3. 이제, 마지막으로 양변에 2를 곱해주어 원하는 적분값(에 근사한 값)을 구할 수 있다.

