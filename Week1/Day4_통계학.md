## 인공지능 통계학 기초



### 통계적 모델링

적절한 가정 위에서 확률 분포를 추정하는 것 (확률 분포 종류는 굉장히 다양하다)



한계 ) 데이터의 개수가 유한하기 때문에 정확한 모집단의 분포를 얻는다는건 사실상 불가능에 가깝다.

목표) 불확실성을 고려한 상태에서 위험성을 최소화하는 방향으로 확률분포를 추정하도록 하자!



#### 모수란?

**모집단의 특성**을 나타내주는 통계치이다. 어떤 **확률분포를 따르느냐에 따라서 다른 모수를 사용**하게 된다.



#### 모수적 방법론

데이터가 특정 확률 분포를 따른다고 가정을 한 후에 그 분포에 대한 모수를 추정하는 방법

Ex) 정규 분포를 따른다고 가정을 하고, 모수를 평균과 분산으로 잡고 추정한다.



#### 비모수 방법론

특정 확률 분포를 따른다는 가정을 하지 않고 모델의 구조와 모수의 개수가 바뀐다.

**주의!** 비모수 방법론이라고해서 모수가 없다는 것이 아니라 무수히 많거나, 모수의 수가 바뀌는 것이다.



#### 확률 분포의 종류

데이터 생성 원리를 기반으로하여 확률 분포를 가정해야하며, 통계적 검정을 통해 모수 추정 후에 진행해야 한다.

무수히 많지만 대표적으로 사용하는 것들은 다음과 같다.

1. 베르누이 분포 

   0 또는 1의 값을 가진다.

2. 카테고리 분포

   데이터가 N개의 이산적인 값을 가진다.

3. 베타 분포

   데이터가 0~1 사이에서 값을 가지는 경우

4. 감마 분포, 로그 정규 분포

   데이터가 0 이상의 값을 가지는 경우

5. 정규 분포, 라플라스 분포

   데이터가 R 전체에서 값을 가지는 경우



#### 모수 추정

정규분포의 모수는 다음과 같다.

<img width="366" alt="스크린샷 2021-08-22 오후 4 15 31" src="https://user-images.githubusercontent.com/88299729/130350521-f284cd6d-082a-417e-a909-e388a0097e4f.png">



분산을 구할 때 N-1로 나누는 이유 : 불편 추정량을 구하기 위해서이다. 기존 모집단을 대표하는 통계값과 기댓값이 같도록 하기 위함



#### 표집 분포

통계량에 대한 확률 분포, 즉 표본 평균과 분산의 확률 분포를 표집분포라고 한다.

표집분포는 N이 커짐에 따라서 점점 정규분포를 따르게 된다.



<img width="655" alt="스크린샷 2021-08-22 오후 4 28 20" src="https://user-images.githubusercontent.com/88299729/130350536-8827c736-d29e-4114-add6-b8d28d1798a3.png">

N이 작을 때에는 양극단에 데이터가 찍힐 수 있으나, N이 커짐에 따라 점점 정규 분포를 따르는 것을 볼 수 있다. (위의 예시는 베르누이 확률 분포를 기반으로 만들어졌다.)

**주의!** 표본 분포와 표집 분포를 헷갈리지 않도록 하자!!



### 최대 가능도 추정법 (Maximum Likelihood Estimation, MLE)

확률분포마다 사용하는 모수가 다르기 때문에 적절한 통계량이 달라지게 된다. 이를 해결하기 위해, 특정 표본으로부터 무엇을 모수로 추정해야 하는지 정하는 방법이 필요하다. 가**장 가능성이 높은 모수를 추정하는 방법이 최대 가능도 추정법**이다.



<img width="404" alt="스크린샷 2021-08-22 오후 4 32 50" src="https://user-images.githubusercontent.com/88299729/130350549-3dc86f9d-730f-4b9c-80ca-a86ae3236b39.png">



모수 *θ* 가 변함에 따라 바뀌는 함수로 생각 할 수 있다. 따라서, 확률로 생각하고 ∑, ∫ 를 통해 1의 값이 구해지지 않는다.



<img width="533" alt="스크린샷 2021-08-22 오후 4 42 11" src="https://user-images.githubusercontent.com/88299729/130350555-d243612b-6db0-48da-b195-11d3fba06318.png">

데이터 집합 X가 **독립적**으로 추출되었을 경우에는 로그 가능도 ***logL* 를 통해 곱셈을 덧셈으로 바꿈**으로서 더 적은 연산량을 확보 할 수 있다. ***logL***을 사용함으로서 연산의 오차 또한 줄어든다.

또한, 경사하강법으로 가능도를 최적화 할 경우 연산 복잡도가 O(n<sup>2</sup>) 에서 O(n) 까지 감소하게 된다.



### 최대가능도(MLE) 추정법 예제 : 정규분포

정규 분포이기에 모수를 평균과 분산으로 한다.

<img width="435" alt="스크린샷 2021-08-22 오후 4 51 18" src="https://user-images.githubusercontent.com/88299729/130350561-bcea4c1e-5aa4-4119-976a-63c702d09862.png">

이 경우 최적의 *θ* 를 찾아야 한다.



<img width="500" alt="스크린샷 2021-08-22 오후 4 53 35" src="https://user-images.githubusercontent.com/88299729/130350574-b1a99d44-da97-4376-b888-3f52ec133b99.png">

이 식을 풀어보면 

<img width="250" alt="스크린샷 2021-08-22 오후 4 54 43" src="https://user-images.githubusercontent.com/88299729/130350577-6f99b36f-9c2e-4312-bfde-b6b95c71d472.png">

이와 같이 풀 수 있다. 이 식을 풀어 각각 평균과 분산에 대해서 미분하여 0이 되는 평균과 분산을 구하면 최대화 되게 된다.



<img width="604" alt="스크린샷 2021-08-22 오후 4 56 55" src="https://user-images.githubusercontent.com/88299729/130350581-f232a242-6eb3-4e72-814f-253b678ac6fb.png">

**주의!** 표본 분산을 구할때는 불편추정량을 구하기 위해 **N-1로 나눴는데**, MLE로 구한 분산을 **N으로 나눈다**. 즉, **MLE는 불편추정량을 보장하지 않는다.**



### 최대가능도 추정법 예제 : 카테고리 분포

카테고리 분포는 이산확률변수에 해당한다. 베르누이에서 0,1 이 아닌 다양한 수로 확장된 분포이다. 이때, 선택된 값을 표현하기 위해 원핫벡터로 표현해준다.



카테고리 분포에서 모수는 P<sub>1</sub>,..., P<sub>d</sub> 1에서부터 d차원까지 어떤 값이 1이 될 확률을 의미하는 통계치이다. 따라서, P<sub>1</sub>,..., P<sub>d</sub> 를 더하게 되면 1이 나온다. 

<img width="529" alt="스크린샷 2021-08-22 오후 6 18 57" src="https://user-images.githubusercontent.com/88299729/130350590-e908ec65-c7f7-4d5b-a26c-74470c222f29.png">



- 만약 해당 데이터 x<sup>i,k</sup>가 1이 될 확률이 0이면,  p<sub>k</sub><sup>x<sup>i,k</sup></sup>는 p<sub>k</sub><sup>0</sup>이 되어 1이 된다.
- 만약 해당 데이터 x<sup>i,k</sup>가 1이 될 확률이 1이면, p<sub>k</sub><sup>x<sup>i,k</sup></sup>는 p<sub>k</sub>가 되어 p<sub>k</sub>가 된다.



<img width="453" alt="스크린샷 2021-08-22 오후 6 27 01" src="https://user-images.githubusercontent.com/88299729/130350604-137f4966-f399-40af-adc7-13a7ce629c20.png">



오른쪽 식을 만족하며 왼쪽 식의 최대화를 구하는 것이 MLE가 된다.

이렇게 제약이 있는 경우, **라그랑주 승수법**을 사용하여 목적식을 수정한다.

<img width="415" alt="스크린샷 2021-08-22 오후 6 32 02" src="https://user-images.githubusercontent.com/88299729/130350613-5bff9833-bd32-4d54-889a-eea4484fdc50.png">

제약식을 양변으로 넘겨준 상태에서 라그랑주 승수에 해당하는 *λ* 를 곱해준 식을 목적식에 더해주어 새로운 목적식을 만들어 준다.

<img width="199" alt="스크린샷 2021-08-22 오후 6 33 29" src="https://user-images.githubusercontent.com/88299729/130350620-4ae54afd-6274-4e50-a8e0-703370595b90.png">

이를 통해 우리는, **분모에 해당하는 값은 데이터 개수 n과 같다**는 것, **p<sub>k</sub> = n<sub>k</sub> / n** 이라는 것, **MLE는 경우의 수를 세어서 전체 중의 비율을 구하는 것**임을 알 수 있다.



### 확률 분포의 거리

손실 함수라는 것은 **기계학습 모델이 학습하는 확률분포와, 데이터에서 관찰되는 확률분포간의 거리**를 통해 유도 할 수 있다.



#### 확률분포간의 거리 구하는 방법

- **총변동 거리(Total Variation Distance, TV)**
- **쿨백-라이블러 발산 (Kullback-Leibler Divergence, KL)**
- **바슈타인 거리(Wasserstein Distance)**



### 쿨백-라이블러 발산

<img width="463" alt="스크린샷 2021-08-22 오후 6 41 34" src="https://user-images.githubusercontent.com/88299729/130350627-e9d2bf36-243b-439b-9125-8e8fcf5a6969.png">

이는, 두개의 항으로 분리 할 수 있다.

<img width="473" alt="스크린샷 2021-08-22 오후 6 43 52" src="https://user-images.githubusercontent.com/88299729/130350628-fbdb453f-67f8-4f5e-b5db-13aa617c33bc.png">

- log*Q*(*x*)의 기댓값 - **크로스 엔트로피** —> 이후 negative를 취한다
- log*P*(*x*)에 대한 기댓값 - **엔트로피**
- 총 두개의 엔트로피 함수로 표현할 수 있다.

정답 Label을 P, 예측을 Q라고 하면, 최대가능도 추정법에서 **손실함수는 쿨백-라이블러 발산의 크로스 엔트로피의 역수**로 표현 할 수 있다.



한마디로, **logL을 최대화 시키는 것**은 **쿨백-라이블러 발산을 최소화** 하는 것과 같다.

